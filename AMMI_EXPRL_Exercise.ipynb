{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "AMMI_EXPRL_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Volviane/Reinforcement-learning/blob/master/AMMI_EXPRL_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UWM-gGi76pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5EbzJ1A78Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/exploration')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDPYhYvR7yKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from riverswim import RiverSwim\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5twn7Qv7yKf",
        "colab_type": "text"
      },
      "source": [
        "# Finite-Horizon MDPs\n",
        "We consider finite horizon problems with horizon $H$. For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
        "\n",
        "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
        "\n",
        "\n",
        "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
        "$$\n",
        "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
        "$$\n",
        "where $V_{H+1}(s) = 0$, for any $s$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l8433mz7yKh",
        "colab_type": "code",
        "outputId": "08fa9497-ba09-475c-84bb-6301b5f20434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "env = RiverSwim(6)\n",
        "H = 10\n",
        "print(\"Reward matrix: \", env.R.shape)\n",
        "print(env.R)\n",
        "print()\n",
        "print(\"Transition matrix: \", env.P.shape)\n",
        "print(\"Transitions probabilities for state s_1:\")\n",
        "print(env.P[1])\n",
        "\n",
        "print(env.P[0,1,1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward matrix:  (6, 2)\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "\n",
            "Transition matrix:  (6, 2, 6)\n",
            "Transitions probabilities for state s_1:\n",
            "[[1.   0.   0.   0.   0.   0.  ]\n",
            " [0.05 0.6  0.35 0.   0.   0.  ]]\n",
            "0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXHdYle7yKm",
        "colab_type": "text"
      },
      "source": [
        "# Backward induction (aka Value Iteration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_KZXn8A7yKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_induction(P, R, H):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "\n",
        "        Returns:\n",
        "            The optimal V-function : array of shape [horizon, s]\n",
        "            The optimal policy\n",
        "        V[h,s] \n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^*(h, s) using the Bellman optimality equation:\n",
        "\n",
        "            V[h, s] = max_a  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            for a in range(A):\n",
        "                tmp =  R[s, a] + np.dot(P[s, a],V[h + 1])\n",
        "                if (a == 0) or (tmp > V[h, s]):\n",
        "                    policy[h, s] = a\n",
        "                    V[h, s] = tmp\n",
        "    return V, policy\n",
        "\n",
        "def policy_evaluation(P, R, H, policy):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "            policy: policy (H,S)-dim matrix\n",
        "\n",
        "        Returns:\n",
        "            The V-function of the given policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^pi(h, s) using the Bellman equation for the policy pi:\n",
        "\n",
        "            a = policy[h,s]\n",
        "            V[h, s] =  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            a = policy[h,s]\n",
        "            V[h, s] = R[s, a] + np.dot(P[s, a,:],V[h + 1,:])\n",
        "            # complete the policy evalution here\n",
        "    return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvmhRCN07yKs",
        "colab_type": "text"
      },
      "source": [
        "Compute solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVjkydge7yKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "d1521f8b-e521-43ce-cc48-6a078d67e876"
      },
      "source": [
        "Vstar, POLstar = backward_induction(env.P, env.R, H)\n",
        "\n",
        "print(\"Optimal policy\")\n",
        "print(np.round(Vstar))\n",
        "\n",
        "print(POLstar)\n",
        "v_policy = policy_evaluation(env.P, env.R, H, POLstar)\n",
        "print(np.abs(Vstar-v_policy).sum())\n",
        "print('V^pi(h, s)\\n',np.round(v_policy))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal policy\n",
            "[[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[[1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 0 1 1 1 1]\n",
            " [0 0 0 1 1 1]\n",
            " [0 0 0 0 1 1]\n",
            " [0 0 0 0 0 1]]\n",
            "0.0\n",
            "V^pi(h, s)\n",
            " [[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0rG4mlU7vCz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "1c19ad2f-5b87-4ac6-c756-107f821b1569"
      },
      "source": [
        "S,A = env.R.shape\n",
        "Phat = np.zeros((S,A,S))\n",
        "Rhat = np.zeros((S,A))\n",
        "\n",
        "N_sa = np.zeros((S,A))\n",
        "N_sas = np.zeros((S,A,S))\n",
        "S_sa = np.zeros((S,A))\n",
        "\n",
        "\n",
        "\n",
        "nb_episodes = 200\n",
        "\n",
        "for ep in range(nb_episodes):\n",
        "    state = env.reset()\n",
        "    for h in range(H):\n",
        "        action = np.random.choice(A)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        N_sa[state,action] +=1\n",
        "        N_sas[state, action, next_state] +=1\n",
        "        S_sa[state, action] +=reward\n",
        "\n",
        "        Rhat[state, action] = S_sa[state, action]/N_sa[state,action]\n",
        "\n",
        "        Phat[state, action, :] = N_sas[state,action,:]/N_sa[state,action]\n",
        "\n",
        "        state = next_state\n",
        "#print(Rhat)\n",
        "#print(Phat)\n",
        "\n",
        "#check the confident interval for the rewards\n",
        "beta_r = np.zeros((S,A))\n",
        "beta_p = np.zeros((S,A))\n",
        "for s in range(S):\n",
        "    for a in range(A):\n",
        "        n = max(N_sa[s,a],1)\n",
        "        beta_r[s,a] = np.sqrt(np.log(S*A*n)/n)\n",
        "        beta_p[s,a] = np.sqrt(S*np.log(S*A*n)/n)\n",
        "\n",
        "# print(Rhat)\n",
        "# print(Rhat+beta_r) \n",
        "# print(Rhat-beta_r) \n",
        "\n",
        "print(Phat)\n",
        "for s in range(S):\n",
        "    for a in range(A):\n",
        "        norm_1 = np.abs(env.P[s,a,:] - Phat[s,a,:]).sum()\n",
        "        print(norm_1, beta_p[s,a])\n",
        "        assert(norm_1 <= beta_p[s,a])\n",
        "                    \n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1.         0.         0.         0.         0.         0.        ]\n",
            "  [0.421875   0.578125   0.         0.         0.         0.        ]]\n",
            "\n",
            " [[1.         0.         0.         0.         0.         0.        ]\n",
            "  [0.02768166 0.59861592 0.37370242 0.         0.         0.        ]]\n",
            "\n",
            " [[0.         1.         0.         0.         0.         0.        ]\n",
            "  [0.         0.07042254 0.5915493  0.33802817 0.         0.        ]]\n",
            "\n",
            " [[0.         0.         1.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.78571429 0.21428571 0.        ]]\n",
            "\n",
            " [[0.         0.         0.         1.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.        ]]\n",
            "\n",
            " [[0.         0.         0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.        ]]]\n",
            "0.0 0.2932989006197667\n",
            "0.043749999999999956 0.28960708554018255\n",
            "0.0 0.4217800698966739\n",
            "0.04740484429065749 0.4113779697612079\n",
            "0.0 0.7551281876158874\n",
            "0.040845070422535185 0.7551281876158874\n",
            "0.0 1.3385661990458504\n",
            "0.37142857142857144 1.4818854755349338\n",
            "0.0 3.0877437541097605\n",
            "1.0 3.8612743879097744\n",
            "1.0 3.8612743879097744\n",
            "1.0 3.8612743879097744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1nCGWuw7yKy",
        "colab_type": "text"
      },
      "source": [
        "## UCRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoWtsh7v7yK0",
        "colab_type": "text"
      },
      "source": [
        "UCRL is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
        "In this setting, the regret is defined as\n",
        "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
        "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
        "\n",
        "The structure of the algorithm is as follow\n",
        "\n",
        "For $k = 1, \\ldots, K$ do<br>\n",
        "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
        "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
        ">> for $h=1, \\ldots, H$<br>\n",
        ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
        ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
        ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
        "\n",
        "<font color='#ed7d31'>Optimistic planning</font>\n",
        "At each episode, UCRL computes the optimal solution by solving the following \"extended\" problem\n",
        "$$\n",
        "V_h^\\star(s) =  \\max_{r \\in B_r(s,a)} r + \\max_{p \\in B_p(s,a)} \\sum_{s'} p(s') V_{h+1}(s') \n",
        "$$\n",
        "where $V_{H+1}(s) = 0$ and $B_r(s,a)$ and $B_p(s,a)$ are confidence intervals on the estimated transitions and rewards:\n",
        "\n",
        "$$\n",
        "B_r(s,a) = \\{ r(s,a):  |r_s, a) - \\hat{r}(s,a)| \\leq \\beta_r(s,a)  \\}\n",
        "$$\n",
        "\n",
        "$$\n",
        "B_p(s,a) = \\{ p(\\cdot|s,a):  ||p(\\cdot|s,a) - \\hat{p}(\\cdot|s,a)||_1 \\leq \\beta_p(s,a)  \\}\n",
        "$$\n",
        "where \n",
        "\n",
        "$$\n",
        "\\beta_r(s, a) = \\sqrt{ \\frac{ \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  } \\\\\n",
        "\\beta_p(s, a) = \\sqrt{ \\frac{  S \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  }\n",
        "$$\n",
        "and where  $N^+(s, a) = \\max(1, N(s, a))$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji7IShLN7yK1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "The following function computes:\n",
        "$$\\max_{x \\in B_p} \\sum_{s'} x(s') V(s') $$\n",
        "where $B_p = [P-\\beta, P+\\beta]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zVp32L7yK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LPprobaH(v, P, beta, verbose=0):\n",
        "    \"\"\"\n",
        "        max_x v^T x\n",
        "        s.t.    0 <= x_i <= 1\n",
        "                \\sum_i |x_i - p_i| \\leq beta\n",
        "                \\sum_i x_i = 1\n",
        "    \"\"\"\n",
        "    sorted_idxs = np.argsort(v)[::-1]\n",
        "\n",
        "    pest = P.copy()\n",
        "    idx = sorted_idxs[0]\n",
        "    pest[idx] = min(1., P[idx] + beta / 2.)\n",
        "    delta = pest.sum()\n",
        "    j = len(P)-1\n",
        "    while delta > 1:\n",
        "        idx_j = sorted_idxs[j]\n",
        "        m = max(0, 1. - delta + pest[idx_j])\n",
        "        delta = delta - pest[idx_j] + m\n",
        "        pest[idx_j] = m\n",
        "        j -= 1\n",
        "    w = np.dot(pest, v)\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk0v7jg67yK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UCRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    V = np.zeros((H + 1, S))\n",
        "\n",
        "    S_sa = np.zeros((S,A))\n",
        "    \n",
        "    delta = 0.1\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute optimistic solution\n",
        "        # 1. compute confidence intervals\n",
        "        N = np.maximum(N_sa, 1)\n",
        "        LOGT = np.log(S * A * N / delta)\n",
        "        beta_r = np.sqrt(LOGT/N) #...\n",
        "        beta_p = np.sqrt(S*LOGT/N)#...\n",
        "        \n",
        "        # 2. run extended value iteration\n",
        "        V.fill(0)\n",
        "        for h in reversed(range(H)):\n",
        "            for s in range(S):\n",
        "                temp = np.zeros((A,))\n",
        "                for a in range(A):\n",
        "                    dotp = LPprobaH(V[h + 1], Phat[s, a], beta_p[s, a])\n",
        "                    temp[a] = Rhat[s,a] + beta_r[s,a] +dotp\n",
        "                V[h, s] = np.max(temp)  #...\n",
        "                policy[h,s] = np.argmax(temp)\n",
        "        \n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Phat, Rhat, N_sa, N_sas)\n",
        "            N_sa[state,action] +=1\n",
        "            N_sas[state, action, next_state] +=1\n",
        "            S_sa[state, action] +=reward\n",
        "            \n",
        "            Rhat[state, action] = S_sa[state, action]/N_sa[state,action]\n",
        "\n",
        "            Phat[state, action, :] = N_sas[state,action,:]/N_sa[state,action]#...\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-UTr3Gq7yK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be6694ae-5104-4b89-cc23-89bcfd5397d6"
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = UCRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running simulation: 0\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.2419620524999999\n",
            "regret[100]: 0.03363150300000001\n",
            "regret[150]: 0.03363150300000001\n",
            "regret[200]: 0.10466703881249992\n",
            "regret[250]: 0.07835019981249997\n",
            "regret[300]: 0.031235826999999994\n",
            "regret[350]: 0.033321987750000004\n",
            "regret[400]: 0.17598918278124995\n",
            "regret[450]: 0.03306478075000002\n",
            "regret[500]: 0.035121323718749986\n",
            "regret[550]: 0.03363150300000001\n",
            "regret[600]: 0.031235826999999994\n",
            "regret[650]: 0.03272564771874997\n",
            "regret[700]: 0.03363150300000001\n",
            "Running simulation: 1\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.34722097893749987\n",
            "regret[100]: 0.031235826999999994\n",
            "regret[150]: 0.03363150300000001\n",
            "regret[200]: 0.06458863575000001\n",
            "regret[250]: 0.03363150300000001\n",
            "regret[300]: 0.03306478075000002\n",
            "regret[350]: 0.03363150300000001\n",
            "regret[400]: 0.031235826999999994\n",
            "regret[450]: 0.03363150300000001\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.03363150300000001\n",
            "regret[600]: 0.03272564771874997\n",
            "regret[650]: 0.05004173775000004\n",
            "regret[700]: 0.03272564771874997\n",
            "Running simulation: 2\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3387837299999999\n",
            "regret[100]: 0.03363150300000001\n",
            "regret[150]: 0.03363150300000001\n",
            "regret[200]: 0.13206049799999997\n",
            "regret[250]: 0.03363150300000001\n",
            "regret[300]: 0.10466703881249992\n",
            "regret[350]: 0.03272564771874997\n",
            "regret[400]: 0.035121323718749986\n",
            "regret[450]: 0.03272564771874997\n",
            "regret[500]: 0.03272564771874997\n",
            "regret[550]: 0.03363150300000001\n",
            "regret[600]: 0.09297454075\n",
            "regret[650]: 0.03363150300000001\n",
            "regret[700]: 0.03272564771874997\n",
            "Running simulation: 3\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3399902904999999\n",
            "regret[100]: 0.035121323718749986\n",
            "regret[150]: 0.0445777933125\n",
            "regret[200]: 0.10615685953125001\n",
            "regret[250]: 0.03363150300000001\n",
            "regret[300]: 0.031235826999999994\n",
            "regret[350]: 0.03363150300000001\n",
            "regret[400]: 0.03272564771874997\n",
            "regret[450]: 0.03363150300000001\n",
            "regret[500]: 0.03363150300000001\n",
            "regret[550]: 0.03363150300000001\n",
            "regret[600]: 0.031235826999999994\n",
            "regret[650]: 0.03363150300000001\n",
            "regret[700]: 0.03363150300000001\n",
            "Running simulation: 4\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.34544283393749986\n",
            "regret[100]: 0.03363150300000001\n",
            "regret[150]: 0.34575234918749986\n",
            "regret[200]: 0.07835019981249997\n",
            "regret[250]: 0.08074587581249998\n",
            "regret[300]: 0.031235826999999994\n",
            "regret[350]: 0.03272564771874997\n",
            "regret[400]: 0.03272564771874997\n",
            "regret[450]: 0.031235826999999994\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.03363150300000001\n",
            "regret[600]: 0.031235826999999994\n",
            "regret[650]: 0.03363150300000001\n",
            "regret[700]: 0.035121323718749986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M49yfZ_S7yLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e572dc0b-399e-45ac-9be1-5627eddb9464"
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "ucrl_regret = mean_regret"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPuG9Er7yLL",
        "colab_type": "text"
      },
      "source": [
        "# Posterior Sampling for RL\n",
        "\n",
        "At each iteration, PSRL samples one MDP from the posterior distribution and run the associated optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gxP6B9r7yLN",
        "colab_type": "text"
      },
      "source": [
        "Implement posterior sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl9YwtfJ7yLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PSRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    reward_prior = [1,1]\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute policy\n",
        "        # 1. sample MDP\n",
        "        R = np.zeros_like(Rhat)\n",
        "        P = np.zeros((S, A, S))\n",
        "        for s in range(S):\n",
        "            for a in range(A):\n",
        "                # sample transition matrix\n",
        "                # P[s, a] follows a dirichlet Dirichlet distribution of parameters N_sas[s, a,:] + 1\n",
        "                P[s, a] = ...\n",
        "\n",
        "                # posterior for Bernoulli rewards\n",
        "                N = N_sa[s, a]\n",
        "                v = N * Rhat[s, a]\n",
        "                a0 = reward_prior[0] + v\n",
        "                b0 = reward_prior[1] + N - v\n",
        "                p = np.random.beta(a=a0, b=b0, size=1).item()\n",
        "                R[s, a] = p\n",
        "        \n",
        "        # 2. compute optimal policy\n",
        "        V, policy = ...\n",
        "        \n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Rhat, N_sa, N_sas)\n",
        "            ...\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT38aVOB7yLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = PSRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQhP7cpn7yLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "psrl_regret = mean_regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phHZWwdJ7yLc",
        "colab_type": "text"
      },
      "source": [
        "Compare algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvcqwaJY7yLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(ucrl_regret, label='UCRL-H')\n",
        "plt.plot(psrl_regret, label='PSRL')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2tQtxL7yLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gradient\n",
        "# reinforce\n",
        "# dqn how it work how to do the update, replay buffer and target Network \n",
        "# ucb\n",
        "# difference between MDP(known dynamic),Bandit(we have only one state : (recommendation problem)) and RL\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test 2\n",
        "\n",
        "# The topics for the second quiz are:\n",
        "\n",
        "# - DQN\n",
        "\n",
        "#    * algorithm\n",
        "\n",
        "#    * issues of naive version (ie correlated samples and oscillation)\n",
        "\n",
        "#         do not consider the other issues mentioned in the slides\n",
        "\n",
        "# - UCB\n",
        "\n",
        "#    * algorithm\n",
        "\n",
        "#    * do the exercise I posted on piazza\n",
        "\n",
        "# - Think about the difference between MAB and RL problem\n",
        "\n",
        " \n",
        "\n",
        "# I removed REINFORCE on purpose."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}